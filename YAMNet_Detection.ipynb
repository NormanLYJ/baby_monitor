{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YAMNet-Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuTtZxN7_1_R"
      },
      "source": [
        "#Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1maMZHUHygnI"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change working directory to be current folder\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/iss/babydetect/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuQzB1FFANv6"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaNolA2wALvr"
      },
      "source": [
        "!pip install tensorflow.io\n",
        "!pip install ffmpeg moviepy\n",
        "!pip install librosa\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install PyAudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ep-q7k_5R-"
      },
      "source": [
        "# Sound classification with YAMNet\n",
        "\n",
        "YAMNet is a deep net that predicts 521 audio event [classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) from the [AudioSet-YouTube corpus](http://g.co/audioset) it was trained on. It employs the\n",
        "[Mobilenet_v1](https://arxiv.org/pdf/1704.04861.pdf) depthwise-separable\n",
        "convolution architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bteu7pfkpt_f"
      },
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import moviepy.editor as mp\n",
        "\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVs3zRrrYmY"
      },
      "source": [
        "Load the Model from TensorFlow Hub.\n",
        "\n",
        "Note: to read the documentation just follow the model's [url](https://tfhub.dev/google/yamnet/1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX8Vzs6EpwMo"
      },
      "source": [
        "# Load the model.\n",
        "yamnet_model = hub.load('YAMNet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWx6tOdtdBP"
      },
      "source": [
        "The labels file will be loaded from the models assets and is present at `model.class_map_path()`.\n",
        "You will load it on the `class_names` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC1p1YR1UQnf"
      },
      "source": [
        "# solution: loading label names\n",
        "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
        "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
        "\n",
        "for name in class_names[:5]:\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFjRwkZ59lU"
      },
      "source": [
        "Add a method to convert a loaded audio is on the proper sample_rate (16K), otherwise it would affect the model's results.\n",
        "\n",
        "Returned wav_data has been normalized to values in [-1.0, 1.0] (as stated in the model's documentation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z5XLZ9k_tXc"
      },
      "source": [
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" read in a waveform file and convert to 16 kHz mono \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents,\n",
        "                                          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEgCobA9bWl"
      },
      "source": [
        "## Preparing the sound file\n",
        "\n",
        "The audio file should be a mono wav file at 16kHz sample rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo9KJb-5zuz1"
      },
      "source": [
        "wav_file_name = './datasets/ESC-50-master/audio/1-187207-A-20.wav'\n",
        "\n",
        "wav_data = load_wav_16k_mono(wav_file_name)\n",
        "\n",
        "# Play the audio file.\n",
        "display.Audio(wav_data, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJuRYFVjJ695"
      },
      "source": [
        "plt.plot(wav_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Xwd4GPuMsB"
      },
      "source": [
        "## Executing the Model\n",
        "\n",
        "Now the easy part: using the data already prepared, you just call the model and get the: scores, embedding and the spectrogram.\n",
        "\n",
        "The score is the main result you will use.\n",
        "The spectrogram you will use to do some visualizations later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJGP6r-At_Jc"
      },
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = yamnet_model(wav_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGGgbVZooiB"
      },
      "source": [
        "scores_np = scores.numpy()\n",
        "spectrogram_np = spectrogram.numpy()\n",
        "infered_class = class_names[scores_np.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {infered_class}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmo7griQprDk"
      },
      "source": [
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.argmax(class_scores)\n",
        "infered_class = class_names[top_class]\n",
        "\n",
        "print(f'The main sound is: {infered_class}')\n",
        "print(f'The embeddings shape: {embeddings.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj2xLf-P_ndS"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "YAMNet also returns some additional information that we can use for visualization.\n",
        "Let's take a look on the Waveform, spectrogram and the top classes inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QSTkmv7wr2M"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the waveform.\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(wav_data)\n",
        "plt.xlim([0, len(wav_data)])\n",
        "\n",
        "# Plot the log-mel spectrogram (returned by the model).\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "\n",
        "# Plot and label the model output scores for the top-scoring classes.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "top_n = 10\n",
        "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
        "\n",
        "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
        "# values from the model documentation\n",
        "patch_padding = (0.025 / 2) / 0.01\n",
        "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
        "# Label the top_N classes.\n",
        "yticks = range(0, top_n, 1)\n",
        "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
        "_ = plt.ylim(-0.5 + np.array([top_n, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ewNyuIidy6x"
      },
      "source": [
        "## ESC-50 dataset\n",
        "\n",
        "The ESC-50 dataset, well described here, is a labeled collection of 2000 environmental audio recordings (each 5 seconds long). The data consists of 50 classes, with 40 examples per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaydsT5pd1a3"
      },
      "source": [
        "_ = tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERAdJNp_nMiC"
      },
      "source": [
        "## Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWFw4HfSnMx1"
      },
      "source": [
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "pd_data = pd.read_csv(esc50_csv)\n",
        "pd_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLVYjC2nbR_"
      },
      "source": [
        "## Filter the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNx_qAVulJoI"
      },
      "source": [
        "my_classes = ['crying_baby', 'others']\n",
        "saved_model_path = './baby_crying_yamnet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFLusfvIpBIg"
      },
      "source": [
        "filtered_pd_crying = pd_data[pd_data.category.isin(['crying_baby'])]\n",
        "print(len(filtered_pd_crying))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfOk8FVdnbdn"
      },
      "source": [
        "map_class_to_id = {'crying_baby':0, 'laughing':1}\n",
        "\n",
        "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
        "\n",
        "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
        "filtered_pd = filtered_pd.assign(target=class_id)\n",
        "\n",
        "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
        "filtered_pd = filtered_pd.assign(filename=full_path)\n",
        "\n",
        "filtered_pd.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr0tbQNeoF1V"
      },
      "source": [
        "## Load the audio files and retrieve embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66QGqgOdoF9r"
      },
      "source": [
        "filenames = filtered_pd['filename']\n",
        "targets = filtered_pd['target']\n",
        "folds = filtered_pd['fold']\n",
        "\n",
        "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
        "main_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1fRuhn8oOvy"
      },
      "source": [
        "def load_wav_for_map(filename, label, fold):\n",
        "  return load_wav_16k_mono(filename), label, fold\n",
        "\n",
        "#main_ds = main_ds.map(lambda a,b,c: tf.py_function(load_wav_for_map, [a, b, c], [tf.float32,tf.int64,tf.int64]))\n",
        "main_ds = main_ds.map(load_wav_for_map)\n",
        "main_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDiKNcWKtWyd"
      },
      "source": [
        "def extract_embedding(wav_data, label, fold):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
        "  num_embeddings = tf.shape(embeddings)[0]\n",
        "\n",
        "  return (embeddings,\n",
        "            tf.repeat(label, num_embeddings),\n",
        "            tf.repeat(fold, num_embeddings))\n",
        "\n",
        "# extract embedding\n",
        "main_ds = main_ds.map(extract_embedding).unbatch()\n",
        "#main_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI78dr5otyvS"
      },
      "source": [
        "cached_ds = main_ds.cache()\n",
        "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
        "\n",
        "# remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXHnoSezBMY"
      },
      "source": [
        "print(train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mob1l4AOhrg7"
      },
      "source": [
        "## Create new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQGpwhgohuoS"
      },
      "source": [
        "new_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), \n",
        "                          dtype=tf.float32,\n",
        "                          name='input_embedding'),\n",
        "\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(my_classes))\n",
        "], name='new_model')\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geQhIXcWiDZ_"
      },
      "source": [
        "new_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv-R5yAZiKb9"
      },
      "source": [
        "history = new_model.fit(train_ds,\n",
        "                       epochs=20,\n",
        "                       validation_data=val_ds,\n",
        "                       callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDDRmUJUiQn7"
      },
      "source": [
        "Lets run the evaluate method on the test data just to be sure there's no overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcCEZkC9iP9S"
      },
      "source": [
        "loss, accuracy = new_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7grEW1BiYV8"
      },
      "source": [
        "## Test your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5fy-UM5iacn"
      },
      "source": [
        "test_laughing_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-155670-A-26.wav')\n",
        "\n",
        "scores, embeddings, spectrogram = yamnet_model(test_laughing_data)\n",
        "result = new_model(embeddings).numpy()\n",
        "print(result)\n",
        "\n",
        "infered_class = my_classes[result.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {infered_class}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVKvnZ9uugBc"
      },
      "source": [
        "## Save a model that can directly take a wav file as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qf94xeVuhVq"
      },
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChBayzW2ujxB"
      },
      "source": [
        "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
        "embedding_extraction_layer = hub.KerasLayer('YAMNet',\n",
        "                                            trainable=False, \n",
        "                                            name='yamnet')\n",
        "\n",
        "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
        "\n",
        "serving_outputs = new_model(embeddings_output)\n",
        "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeiM7eG0u0oE"
      },
      "source": [
        "tf.keras.utils.plot_model(serving_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f534dSEhuu4C"
      },
      "source": [
        "## Test new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AALu0uwwNzJ3"
      },
      "source": [
        "#test_laughing_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-155670-A-26.wav')\n",
        "#test_crying_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-167077-A-20.wav')\n",
        "aaa = load_wav_16k_mono('./datasets/Babies_Crying.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCQeKRqHl3jH"
      },
      "source": [
        "# loading new model\n",
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEPeqxbeux9T"
      },
      "source": [
        "# test in new data file\n",
        "reloaded_results = reloaded_model(aaa)\n",
        "print(reloaded_results)\n",
        "\n",
        "baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
        "print(f'The main sound is: {baby_sound}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQpPwMgeNLdj"
      },
      "source": [
        "## Loading video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4aZZv0-NXYI"
      },
      "source": [
        "my_clip = mp.VideoFileClip(r\"./datasets/Babies_Crying.mp4\")\n",
        "\n",
        "my_clip.audio.write_audiofile(r\"./datasets/Babies_Crying.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDB91Awcgi41"
      },
      "source": [
        "## Read audio file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnoWXAZugjE0"
      },
      "source": [
        "sample_rate = 16000\n",
        "rate = 44100\n",
        "\n",
        "duration = len(aaa)/sample_rate\n",
        "\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "\n",
        "for i in range(0, int(duration), 5):\n",
        "  start = i*sample_rate\n",
        "  end   = (i+5)*sample_rate\n",
        "  print('duration from {:d} -- {:d}'.format(i, i+5))\n",
        "\n",
        "  wav_data = aaa[start:end]\n",
        "  print(wav_data.dtype)\n",
        "\n",
        "  reloaded_results = reloaded_model(wav_data)\n",
        "  baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
        "  print(f'The main sound is: {baby_sound}')\n",
        "\n",
        "  filename = 'clip-{:d}.wav'.format(i)\n",
        "\n",
        "  data = np.random.uniform(-1, 1, size=(rate * 10, 2))\n",
        "  sf.write(filename, wav_data, sample_rate, subtype='PCM_24')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-reZmAFgH0CC"
      },
      "source": [
        "## Real-Time audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_XFifg_H4VO"
      },
      "source": [
        "import pyaudio\n",
        "\n",
        "p = pyaudio.PyAudio()\n",
        "\n",
        "print(p.get_device_count())\n",
        "\n",
        "FORMAT          = pyaudio.paInt16\n",
        "CHANNELS        = 1\n",
        "RATE            = 44100\n",
        "RECORD_SECONDS  = 5\n",
        "CHUNK           = int(RATE/20)\n",
        "\n",
        "stream = p.open(format=FORMAT,\n",
        "                channels=CHANNELS,\n",
        "                rate=RATE,\n",
        "                input = True,\n",
        "                frames_per_buffer=CHUNK)\n",
        "\n",
        "while True:\n",
        "    frames = []\n",
        "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
        "        frames.append(np.fromstring(data, dtype=np.float32))\n",
        "    npdata = np.hstack(frames)\n",
        "\n",
        "    wav_data = AudioClip.from_np(npdata, RATE)\n",
        "\n",
        "    #check using model\n",
        "    reloaded_results = reloaded_model(wav_data)\n",
        "    baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
        "    print(f'The main sound is: {baby_sound}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}