{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuTtZxN7_1_R"
   },
   "source": [
    "#Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1maMZHUHygnI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Change working directory to be current folder\n",
    "import os\n",
    "os.chdir('/content/gdrive/My Drive/iss/babydetect/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuQzB1FFANv6"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaNolA2wALvr"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow.io\n",
    "!pip install ffmpeg moviepy\n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install PyAudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2ep-q7k_5R-"
   },
   "source": [
    "# Sound classification with YAMNet\n",
    "\n",
    "YAMNet is a deep net that predicts 521 audio event [classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) from the [AudioSet-YouTube corpus](http://g.co/audioset) it was trained on. It employs the\n",
    "[Mobilenet_v1](https://arxiv.org/pdf/1704.04861.pdf) depthwise-separable\n",
    "convolution architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bteu7pfkpt_f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import moviepy.editor as mp\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSVs3zRrrYmY"
   },
   "source": [
    "Load the Model from TensorFlow Hub.\n",
    "\n",
    "Note: to read the documentation just follow the model's [url](https://tfhub.dev/google/yamnet/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VX8Vzs6EpwMo"
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "yamnet_model = hub.load('YAMNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxWx6tOdtdBP"
   },
   "source": [
    "The labels file will be loaded from the models assets and is present at `model.class_map_path()`.\n",
    "You will load it on the `class_names` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC1p1YR1UQnf"
   },
   "outputs": [],
   "source": [
    "# solution: loading label names\n",
    "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
    "\n",
    "for name in class_names[:5]:\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSFjRwkZ59lU"
   },
   "source": [
    "Add a method to convert a loaded audio is on the proper sample_rate (16K), otherwise it would affect the model's results.\n",
    "\n",
    "Returned wav_data has been normalized to values in [-1.0, 1.0] (as stated in the model's documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4Z5XLZ9k_tXc"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" read in a waveform file and convert to 16 kHz mono \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    \n",
    "    wav, sample_rate = tf.audio.decode_wav(file_contents,\n",
    "                                          desired_channels=1)\n",
    "    \n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    \n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZEgCobA9bWl"
   },
   "source": [
    "## Preparing the sound file\n",
    "\n",
    "The audio file should be a mono wav file at 16kHz sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo9KJb-5zuz1"
   },
   "outputs": [],
   "source": [
    "wav_file_name = './datasets/ESC-50-master/audio/1-187207-A-20.wav'\n",
    "\n",
    "wav_data = load_wav_16k_mono(wav_file_name)\n",
    "\n",
    "# Play the audio file.\n",
    "display.Audio(wav_data, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJuRYFVjJ695"
   },
   "outputs": [],
   "source": [
    "plt.plot(wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_Xwd4GPuMsB"
   },
   "source": [
    "## Executing the Model\n",
    "\n",
    "Now the easy part: using the data already prepared, you just call the model and get the: scores, embedding and the spectrogram.\n",
    "\n",
    "The score is the main result you will use.\n",
    "The spectrogram you will use to do some visualizations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJGP6r-At_Jc"
   },
   "outputs": [],
   "source": [
    "# Run the model, check the output.\n",
    "scores, embeddings, spectrogram = yamnet_model(wav_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vmo7griQprDk"
   },
   "outputs": [],
   "source": [
    "class_scores = tf.reduce_mean(scores, axis=0)\n",
    "top_class = tf.argmax(class_scores)\n",
    "infered_class = class_names[top_class]\n",
    "\n",
    "print(f'The main sound is: {infered_class}')\n",
    "print(f'The embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj2xLf-P_ndS"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "YAMNet also returns some additional information that we can use for visualization.\n",
    "Let's take a look on the Waveform, spectrogram and the top classes inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QSTkmv7wr2M"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(wav_data)\n",
    "plt.xlim([0, len(wav_data)])\n",
    "\n",
    "# Plot the log-mel spectrogram (returned by the model).\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
    "\n",
    "# Plot and label the model output scores for the top-scoring classes.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "top_n = 10\n",
    "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "\n",
    "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
    "# values from the model documentation\n",
    "patch_padding = (0.025 / 2) / 0.01\n",
    "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
    "# Label the top_N classes.\n",
    "yticks = range(0, top_n, 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([top_n, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ewNyuIidy6x"
   },
   "source": [
    "## ESC-50 dataset\n",
    "\n",
    "The ESC-50 dataset, well described here, is a labeled collection of 2000 environmental audio recordings (each 5 seconds long). The data consists of 50 classes, with 40 examples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaydsT5pd1a3"
   },
   "outputs": [],
   "source": [
    "_ = tf.keras.utils.get_file('esc-50.zip',\n",
    "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='datasets',\n",
    "                        extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERAdJNp_nMiC"
   },
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWFw4HfSnMx1"
   },
   "outputs": [],
   "source": [
    "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
    "base_data_path = './datasets/ESC-50-master/audio/'\n",
    "\n",
    "pd_data = pd.read_csv(esc50_csv)\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkLVYjC2nbR_"
   },
   "source": [
    "## Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YNx_qAVulJoI"
   },
   "outputs": [],
   "source": [
    "my_classes = ['crying_baby', 'laughing']\n",
    "saved_model_path = './baby_crying_yamnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfOk8FVdnbdn"
   },
   "outputs": [],
   "source": [
    "map_class_to_id = {'crying_baby':0, 'laughing':1}\n",
    "\n",
    "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
    "\n",
    "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
    "filtered_pd = filtered_pd.assign(target=class_id)\n",
    "\n",
    "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
    "filtered_pd = filtered_pd.assign(filename=full_path)\n",
    "\n",
    "filtered_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr0tbQNeoF1V"
   },
   "source": [
    "## Load the audio files and retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66QGqgOdoF9r"
   },
   "outputs": [],
   "source": [
    "filenames = filtered_pd['filename']\n",
    "targets = filtered_pd['target']\n",
    "folds = filtered_pd['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1fRuhn8oOvy"
   },
   "outputs": [],
   "source": [
    "def load_wav_for_map(filename, label, fold):\n",
    "  return load_wav_16k_mono(filename), label, fold\n",
    "\n",
    "#main_ds = main_ds.map(lambda a,b,c: tf.py_function(load_wav_for_map, [a, b, c], [tf.float32,tf.int64,tf.int64]))\n",
    "main_ds = main_ds.map(load_wav_for_map)\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDiKNcWKtWyd"
   },
   "outputs": [],
   "source": [
    "def extract_embedding(wav_data, label, fold):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "#main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI78dr5otyvS"
   },
   "outputs": [],
   "source": [
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inXHnoSezBMY"
   },
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mob1l4AOhrg7"
   },
   "source": [
    "## Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQGpwhgohuoS"
   },
   "outputs": [],
   "source": [
    "new_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), \n",
    "                          dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='new_model')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geQhIXcWiDZ_"
   },
   "outputs": [],
   "source": [
    "new_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dv-R5yAZiKb9"
   },
   "outputs": [],
   "source": [
    "history = new_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDDRmUJUiQn7"
   },
   "source": [
    "Lets run the evaluate method on the test data just to be sure there's no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcCEZkC9iP9S"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = new_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7grEW1BiYV8"
   },
   "source": [
    "## Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5fy-UM5iacn"
   },
   "outputs": [],
   "source": [
    "test_laughing_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-155670-A-26.wav')\n",
    "\n",
    "scores, embeddings, spectrogram = yamnet_model(test_laughing_data)\n",
    "result = new_model(embeddings).numpy()\n",
    "print(result)\n",
    "\n",
    "infered_class = my_classes[result.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {infered_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVKvnZ9uugBc"
   },
   "source": [
    "## Save a model that can directly take a wav file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qf94xeVuhVq"
   },
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChBayzW2ujxB"
   },
   "outputs": [],
   "source": [
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "embedding_extraction_layer = hub.KerasLayer('YAMNet',\n",
    "                                            trainable=False, \n",
    "                                            name='yamnet')\n",
    "\n",
    "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "\n",
    "serving_outputs = new_model(embeddings_output)\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "serving_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeiM7eG0u0oE"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(serving_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f534dSEhuu4C"
   },
   "source": [
    "## Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "AALu0uwwNzJ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'> (4084480,)\n"
     ]
    }
   ],
   "source": [
    "#test_laughing_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-155670-A-26.wav')\n",
    "#test_crying_data = load_wav_16k_mono('./datasets/ESC-50-master/audio/4-167077-A-20.wav')\n",
    "aaa = load_wav_16k_mono('./datasets/Babies_Crying.wav')\n",
    "print(aaa.dtype, aaa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JCQeKRqHl3jH"
   },
   "outputs": [],
   "source": [
    "# loading new model\n",
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "WEPeqxbeux9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.9237363  -0.82834846], shape=(2,), dtype=float32)\n",
      "The main sound is: crying_baby\n"
     ]
    }
   ],
   "source": [
    "# test in new data file\n",
    "reloaded_results = reloaded_model(aaa)\n",
    "print(reloaded_results)\n",
    "\n",
    "baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
    "print(f'The main sound is: {baby_sound}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQpPwMgeNLdj"
   },
   "source": [
    "## Loading video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "z4aZZv0-NXYI"
   },
   "outputs": [],
   "source": [
    "#my_clip = mp.VideoFileClip(r\"./datasets/Babies_Crying.mp4\")\n",
    "\n",
    "#my_clip.audio.write_audiofile(r\"./datasets/Babies_Crying.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDB91Awcgi41"
   },
   "source": [
    "## Read audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "MnoWXAZugjE0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 255.28s\n",
      "duration from 0 -- 5\n",
      "The main sound is: laughing\n",
      "duration from 5 -- 10\n",
      "The main sound is: laughing\n",
      "duration from 10 -- 15\n",
      "The main sound is: crying_baby\n",
      "duration from 15 -- 20\n",
      "The main sound is: crying_baby\n",
      "duration from 20 -- 25\n",
      "The main sound is: laughing\n",
      "duration from 25 -- 30\n",
      "The main sound is: laughing\n",
      "duration from 30 -- 35\n",
      "The main sound is: crying_baby\n",
      "duration from 35 -- 40\n",
      "The main sound is: laughing\n",
      "duration from 40 -- 45\n",
      "The main sound is: laughing\n",
      "duration from 45 -- 50\n",
      "The main sound is: crying_baby\n",
      "duration from 50 -- 55\n",
      "The main sound is: crying_baby\n",
      "duration from 55 -- 60\n",
      "The main sound is: laughing\n",
      "duration from 60 -- 65\n",
      "The main sound is: laughing\n",
      "duration from 65 -- 70\n",
      "The main sound is: crying_baby\n",
      "duration from 70 -- 75\n",
      "The main sound is: laughing\n",
      "duration from 75 -- 80\n",
      "The main sound is: crying_baby\n",
      "duration from 80 -- 85\n",
      "The main sound is: laughing\n",
      "duration from 85 -- 90\n",
      "The main sound is: crying_baby\n",
      "duration from 90 -- 95\n",
      "The main sound is: laughing\n",
      "duration from 95 -- 100\n",
      "The main sound is: crying_baby\n",
      "duration from 100 -- 105\n",
      "The main sound is: crying_baby\n",
      "duration from 105 -- 110\n",
      "The main sound is: crying_baby\n",
      "duration from 110 -- 115\n",
      "The main sound is: crying_baby\n",
      "duration from 115 -- 120\n",
      "The main sound is: laughing\n",
      "duration from 120 -- 125\n",
      "The main sound is: crying_baby\n",
      "duration from 125 -- 130\n",
      "The main sound is: crying_baby\n",
      "duration from 130 -- 135\n",
      "The main sound is: crying_baby\n",
      "duration from 135 -- 140\n",
      "The main sound is: crying_baby\n",
      "duration from 140 -- 145\n",
      "The main sound is: laughing\n",
      "duration from 145 -- 150\n",
      "The main sound is: crying_baby\n",
      "duration from 150 -- 155\n",
      "The main sound is: crying_baby\n",
      "duration from 155 -- 160\n",
      "The main sound is: crying_baby\n",
      "duration from 160 -- 165\n",
      "The main sound is: crying_baby\n",
      "duration from 165 -- 170\n",
      "The main sound is: laughing\n",
      "duration from 170 -- 175\n",
      "The main sound is: laughing\n",
      "duration from 175 -- 180\n",
      "The main sound is: laughing\n",
      "duration from 180 -- 185\n",
      "The main sound is: laughing\n",
      "duration from 185 -- 190\n",
      "The main sound is: laughing\n",
      "duration from 190 -- 195\n",
      "The main sound is: laughing\n",
      "duration from 195 -- 200\n",
      "The main sound is: laughing\n",
      "duration from 200 -- 205\n",
      "The main sound is: laughing\n",
      "duration from 205 -- 210\n",
      "The main sound is: laughing\n",
      "duration from 210 -- 215\n",
      "The main sound is: laughing\n",
      "duration from 215 -- 220\n",
      "The main sound is: crying_baby\n",
      "duration from 220 -- 225\n",
      "The main sound is: laughing\n",
      "duration from 225 -- 230\n",
      "The main sound is: laughing\n",
      "duration from 230 -- 235\n",
      "The main sound is: laughing\n",
      "duration from 235 -- 240\n",
      "The main sound is: laughing\n",
      "duration from 240 -- 245\n",
      "The main sound is: laughing\n",
      "duration from 245 -- 250\n",
      "The main sound is: laughing\n",
      "duration from 250 -- 255\n",
      "The main sound is: laughing\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "\n",
    "duration = len(aaa)/sample_rate\n",
    "\n",
    "print(f'Total duration: {duration:.2f}s')\n",
    "\n",
    "for i in range(0, int(duration), 5):\n",
    "  start = i*sample_rate\n",
    "  end   = (i+5)*sample_rate\n",
    "  print('duration from {:d} -- {:d}'.format(i, i+5))\n",
    "\n",
    "  wav_data = aaa[start:end]\n",
    "  reloaded_results = reloaded_model(wav_data)\n",
    "\n",
    "  baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
    "  print(f'The main sound is: {baby_sound}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-reZmAFgH0CC"
   },
   "source": [
    "## Real-Time audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "0_XFifg_H4VO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "tf.Tensor([-1.5416976  4.2150908], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.3106263  3.677208 ], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.472271  4.151361], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.150538   2.5794582], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.6066098  4.1252203], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.4608643  3.9879856], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n",
      "1\n",
      "2\n",
      "tf.Tensor([-1.4848602  4.0876384], shape=(2,), dtype=float32)\n",
      "The main sound is: laughing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-903bddf69c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRATE\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mCHUNK\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mRECORD_SECONDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#if you want to hear your voice while recording\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Apps\\Anaconda3\\envs\\baby\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import wave\n",
    "import pyaudio\n",
    "import scipy\n",
    "from moviepy.audio import AudioClip\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# sample format\n",
    "FORMAT          = pyaudio.paInt16\n",
    "# mono, change to 2 if you want stereo\n",
    "CHANNELS        = 1\n",
    "# 44100 samples per second\n",
    "RATE            = 16000\n",
    "# record period\n",
    "RECORD_SECONDS  = 3\n",
    "# set the chunk size of 1024 samples\n",
    "CHUNK           = 4096 * 2\n",
    "\n",
    "min_frames_to_process = int(RATE * 2.5)\n",
    "\n",
    "chunks_required = int(np.ceil(min_frames_to_process // CHUNK))\n",
    "\n",
    "# the file name output you want to record into\n",
    "filename = \"recorded.wav\"\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                #output=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "while True:\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        \n",
    "        #if you want to hear your voice while recording\n",
    "        #stream.write(data)\n",
    "        \n",
    "        #result = np.fromstring(data, dtype=np.float16)\n",
    "        #frames.append(result)\n",
    "        \n",
    "        frames.append(data)\n",
    "    #npdata = np.hstack(frames)\n",
    "    \n",
    "    #npdata = np.array(npdata, dtype=np.float32)\n",
    "\n",
    "    #wav_data = AudioClip.from_np(npdata, RATE)\n",
    "\n",
    "    #check using model\n",
    "    reloaded_results = reloaded_model(wave_arr)\n",
    "    print(reloaded_results)\n",
    "    baby_sound = my_classes[tf.argmax(reloaded_results)]\n",
    "    print(f'The main sound is: {baby_sound}')\n",
    "    \n",
    "    #c = cv.WaitKey(7) % 0x100\n",
    "    #if c == 27 or c == 10:\n",
    "    #    break\n",
    "\n",
    "# stop and close stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "# terminate pyaudio object\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(filename, \"wb\")\n",
    "# set the channels\n",
    "wf.setnchannels(CHANNELS)\n",
    "# set the sample format\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "# set the sample rate\n",
    "wf.setframerate(RATE)\n",
    "# write the frames as bytes\n",
    "wf.writeframes(b\"\".join(frames))\n",
    "# close the file\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YAMNet-Detection.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
